# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HiNr_hbBWMIrQ7bHBIfXNeF_Z_EcJZtx
"""

# Commented out IPython magic to ensure Python compatibility.
#the KNN algorithm
import numpy as np
# %matplotlib inline      
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2
import sklearn


euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))
manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)
minkowski_p3 = lambda x1, x2: np.cbrt(np.sum((abs(x1 - x2))**3, axis=-1))
#cosine_similarity = lambda a, b: np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))

class KNN:
    def __init__(self, K, dist_fn):
        self.dist_fn = dist_fn
        self.K = K
        return
    
    def fit(self, x, y):
        ''' Store the training data using this method as it is a lazy learner'''
        self.x = x
        self.y = y
        self.C = np.max(y) + 1
        self.C = int(self.C.item())
        return self

    def accuracy(self, TestData, outputset):
      rightLabel= 0
      WrongLabel = 0
      for i in range(len(TestData)):
          if TestData[i][0] == outputset[i]:
              rightLabel +=1
          else:
              WrongLabel +=1
      return [(rightLabel/len(outputset)*100), WrongLabel]

    #get the weights for closest neighbors
    def CalVotes(self,data):
        for i in data:
            vote = 1/i[1]
            i.append(vote)
        data.sort(key=itemgetter(2))
        return data[0][0]

    # KNN Algorithm'''
        

    def predict(self, x_test):
        ''' Makes a prediction using the stored training data and the test data given as argument'''

        num_test = x_test.shape[0]
        #calculate distance between the training & test samples and returns an array of shape [num_test, num_train]
        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])

        #ith-row of knns stores the indices of k closest training samples to the ith-test sample 
        knns = np.zeros((num_test, self.K), dtype=int)
        #ith-row of y_prob has the probability distribution over C classes
        y_prob = np.zeros((num_test, self.C))
        for i in range(num_test):
            knns[i,:] = np.argsort(distances[i])[:self.K]
            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C) #counts the number of instances of each class in the K-closest training samples
        #y_prob /= np.sum(y_prob, axis=-1, keepdims=True)
        #simply divide by K to get a probability distribution
        y_prob /= self.K

        # result = CalVotes(knns)
        return y_prob, knns

"""# Evaluation Function"""

def evaluate_acc(true_labels, target_labels):
  return np.sum(true_labels == target_labels)/true_labels.shape[0]

"""# Dataset Pre-Processing




"""

#cleaning the csv files -> converting them to numpy arrays
np.random.seed(1346)

breast_cancer_panda = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data',header=None)# modify the header to none since no col name in the dataset
breast_cancer_panda = breast_cancer_panda[~breast_cancer_panda.eq('?').any(1)] 
breast_cancer_np = breast_cancer_panda.to_numpy().astype(np.float)
hepatitis_panda = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data',header=None)# modify the header to none since no col name in the dataset
hepatitis_panda = hepatitis_panda[~hepatitis_panda.eq('?').any(1)] 
hepatitis_np = hepatitis_panda.to_numpy().astype(np.float)
#splitting the numpy 2d array to obtain the data_points and the labels as two sets of data
#this way the data format matches the input data format required by the KNN algorithm in class
breast_x = breast_cancer_np[:,1:10]
for c in range(breast_x.shape[1]):
  breast_x[:,c] = (breast_x[:,c] - breast_x[:,c].min())/(breast_x[:,c].max() - breast_x[:,c].min())   # rescaling the data st all data distributing in [0,1]
breast_y = breast_cancer_np[:,10].astype(int)
for i in range(breast_y.shape[0]):    # change the class 2 and 4 to 1 and 2 
  breast_y[i] = int(breast_y[i]/2)

hepatitis_x = hepatitis_np[:,1:]
for c in range(hepatitis_x.shape[1]):
  hepatitis_x[:,c] = (hepatitis_x[:,c] - hepatitis_x[:,c].min())/(hepatitis_x[:,c].max() - hepatitis_x[:,c].min())   # rescaling the data st all data distributing in [0,1]
hepatitis_y = hepatitis_np[:,0].astype(int)


#feature selection
breast_x_fs = SelectKBest(score_func=chi2,k=2).fit_transform(breast_x,breast_y)
hepatitis_x_fs = SelectKBest(score_func=chi2,k=4).fit_transform(hepatitis_x,hepatitis_y)

#partitioning the two datasets into training_set and test_set
breast_data_num = breast_x.shape[0]
hepatitis_data_num = hepatitis_x.shape[0]
breast_partition = int(breast_data_num*0.8)
hepatitis_partition = int(hepatitis_data_num*0.8)
breast_inds = np.random.permutation(breast_data_num)
hepatitis_inds = np.random.permutation(hepatitis_data_num)

breast_x_train, breast_y_train = breast_x[breast_inds[:breast_partition]], breast_y[breast_inds[:breast_partition]]
breast_x_test, breast_y_test = breast_x[breast_inds[breast_partition:]], breast_y[breast_inds[breast_partition:]]

hepatitis_x_train, hepatitis_y_train = hepatitis_x[hepatitis_inds[:hepatitis_partition]], hepatitis_y[hepatitis_inds[:hepatitis_partition]]
hepatitis_x_test, hepatitis_y_test = hepatitis_x[hepatitis_inds[hepatitis_partition:]], hepatitis_y[hepatitis_inds[hepatitis_partition:]]



#partitioning the two datasets into traing set and test set after feature selection
breast_x_train_fs = breast_x_fs[breast_inds[:breast_partition]]
breast_x_test_fs = breast_x_fs[breast_inds[breast_partition:]]

hepatitis_x_train_fs = hepatitis_x_fs[hepatitis_inds[:hepatitis_partition]]
hepatitis_x_test_fs = hepatitis_x_fs[hepatitis_inds[hepatitis_partition:]]

"""# Visualization of the breast cancer data"""

plt.scatter(breast_x_train[:,0], breast_x_train[:,1], c=breast_y_train, marker='o', label='train')
plt.scatter(breast_x_test[:,0], breast_x_test[:,1], c=breast_y_test, marker='s', label='test')
plt.legend()
plt.ylabel('Uniformity of Cell Size')
plt.xlabel('Clump Thickness')

"""# Visualization of the  hepatitis data"""

plt.scatter(hepatitis_x_train[:,0], hepatitis_x_train[:,1], c=hepatitis_y_train, marker='o', label='train')
plt.scatter(hepatitis_x_test[:,0], hepatitis_x_test[:,1], c=hepatitis_y_test, marker='s', label='test')
plt.legend()
plt.ylabel('Sex')
plt.xlabel('Age')

"""#Plot the distribution of features of cancer dataset

"""

#plot the distribution of features of cancer dataset
feature_dict = {'Clump Thckness': breast_x[:, 0], 
                'Uniformity of Cell Size': breast_x[:, 1], 
                'Uniformity of Cell Shape': breast_x[:, 2],
                'Marginal Adhesion': breast_x[:, 3],
                'Single Epithelial Cell Size': breast_x[:,4],
                'Bare Nuclei': breast_x[:, 5],
                'Bland Chromatin': breast_x[:, 6],
                'Normal Nucleoli': breast_x[:, 7],
                'Mitoses': breast_x[:, 8]}
plt.hist(breast_y)
plt.xlabel('Class')
plt.show()  
benign = np.count_nonzero(breast_y==2)
malignant = np.count_nonzero(breast_y==4)
print(f'{breast_y.shape[0]} valid instances in hepatitis dataset, {benign} instances are classified as DIE, {malignant} instances are classified as LIVE')

for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Standard deviation of {name}: {np.std(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""# Plot the distribution for the first class of breast cancer dataset

From this, we can find the relevence between features and class
"""

first_class=breast_cancer_np[breast_cancer_np[...,-1]==2] # get the data which have class 2

breast_x_f = first_class[:,1:10]
breast_y_f = first_class[:,10].astype(int)
#plot the distribution of features of the first class
feature_dict = {'Clump Thckness': first_class[:, 1], 
                'Uniformity of Cell Size': first_class[:, 2], 
                'Uniformity of Cell Shape': first_class[:, 3],
                'Marginal Adhesion': first_class[:, 4],
                'Single Epithelial Cell Size': first_class[:, 5],
                'Bare Nuclei': first_class[:, 6],
                'Bland Chromatin': first_class[:, 7],
                'Normal Nucleoli': first_class[:, 8],
                'Mitoses': first_class[:, 9]}
plt.hist(breast_y_f)
plt.xlabel('Class')
plt.show()  
for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Variance of {name}: {np.var(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""#Plot the distribution for the second class of breast cancer dataset

From this, we can find the relevence between features and class
"""

second_class=breast_cancer_np[breast_cancer_np[...,-1]==4] # get the data which have class 4

breast_x_s = second_class[:,1:10]
breast_y_s = second_class[:,10].astype(int)
#plot the distribution of features of the second class
feature_dict = {'Clump Thckness': second_class[:, 1], 
                'Uniformity of Cell Size': second_class[:, 2], 
                'Uniformity of Cell Shape': second_class[:, 3],
                'Marginal Adhesion': second_class[:, 4],
                'Single Epithelial Cell Size': second_class[:, 5],
                'Bare Nuclei': second_class[:, 6],
                'Bland Chromatin': second_class[:, 7],
                'Normal Nucleoli': second_class[:, 8],
                'Mitoses': second_class[:, 9]}
plt.hist(breast_y_s)
plt.xlabel('Class')
plt.show()  
          
for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Variance of {name}: {np.var(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""# Plot the distribution of each class in hepatitis dataset

We can see the total distribution of each feature. 
"""

plt.hist(hepatitis_y)
plt.xlabel('Class')
plt.show()
die = np.count_nonzero(hepatitis_y==1)
live = np.count_nonzero(hepatitis_y==2)
print(f'{hepatitis_y.shape[0]} valid instances in hepatitis dataset, {die} instances are classified as DIE, {live} instances are classified as LIVE')

#plot the distribution of features of the first class
feature_dict = {'Age': hepatitis_x[:, 0], 
                'Sex': hepatitis_x[:, 1], 
                'Steroid': hepatitis_x[:, 2],
                'Antivirals': hepatitis_x[:, 3],
                'Fatigue': hepatitis_x[:, 4],
                'Malaise': hepatitis_x[:, 5],
                'Anorexia': hepatitis_x[:, 6],
                'Liver_Big': hepatitis_x[:, 7],
                'Liver_Firm': hepatitis_x[:, 8],
                'Spleen_Palpable': hepatitis_x[:, 9],
                'Spiders': hepatitis_x[:, 10],
                'Ascites': hepatitis_x[:, 11],
                'Varices': hepatitis_x[:, 12],
                'Bilirubin': hepatitis_x[:, 13],
                'Alk_Phosphate': hepatitis_x[:, 14],
                'Sgot': hepatitis_x[:, 15],
                'Albumin': hepatitis_x[:, 16],
                'Protime': hepatitis_x[:, 17],
                'Histology': hepatitis_x[:, 18],
                }

for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Variance of {name}: {np.var(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""# Plot the distribution for the first class of hepatitis dataset

From this, we can find the relevence between features and class
"""

first_class=hepatitis_np[hepatitis_np[...,0]==1] # get the data which have class 1

first_hepatitis_x = first_class[:,1:]
first_hepatitis_y = first_class[:,0].astype(int)
#plot the distribution of features of the first class
feature_dict = {'Age': first_class[:, 1], 
                'Sex': first_class[:, 2], 
                'Steroid': first_class[:, 3],
                'Antivirals': first_class[:, 4],
                'Fatigue': first_class[:, 5],
                'Malaise': first_class[:, 6],
                'Anorexia': first_class[:, 7],
                'Liver_Big': first_class[:, 8],
                'Liver_Firm': first_class[:, 9],
                'Spleen_Palpable': first_class[:, 10],
                'Spiders': first_class[:, 11],
                'Ascites': first_class[:, 12],
                'Varices': first_class[:, 13],
                'Bilirubin': first_class[:, 14],
                'Alk_Phosphate': first_class[:, 15],
                'Sgot': first_class[:, 16],
                'Albumin': first_class[:, 17],
                'Protime': first_class[:, 18],
                'histology': first_class[:, 19]
                }
plt.hist(first_hepatitis_y)
plt.xlabel('Class')
plt.show()  
for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Variance of {name}: {np.var(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""# Plot the distribution for the seond class of hepatitis dataset

From this, we can find the relevence between features and class
"""

second_class=hepatitis_np[hepatitis_np[...,0]==2] # get the data which have class 2

second_hepatitis_x = second_class[:,1:]
second_hepatitis_y = second_class[:,0].astype(int)
#plot the distribution of features of the first class
feature_dict = {'Age': second_class[:, 1], 
                'Sex': second_class[:, 2], 
                'Steroid': second_class[:, 3],
                'Antivirals': second_class[:, 4],
                'Fatigue': second_class[:, 5],
                'Malaise': second_class[:, 6],
                'Anorexia': second_class[:, 7],
                'Liver_Big': second_class[:, 8],
                'Liver_Firm': second_class[:, 9],
                'Spleen_Palpable': second_class[:, 10],
                'Spiders': second_class[:, 11],
                'Ascites': second_class[:, 12],
                'Varices': second_class[:, 13],
                'Bilirubin': second_class[:, 14],
                'Alk_Phosphate': second_class[:, 15],
                'Sgot': second_class[:, 16],
                'Albumin': second_class[:, 17],
                'Protime': second_class[:, 18],
                'histology': second_class[:, 19]
                }
plt.hist(second_hepatitis_y)
plt.xlabel('Class')
plt.show()  
for name, data in feature_dict.items():
  plt.hist(data)
  plt.xlabel(name)
  plt.show()
  print(f"Variance of {name}: {np.var(data)}")
  print(f"Mean of Clump {name}: {np.mean(data)}")
  print(f"Median of Clump {name}: {np.median(data)}")

"""# Different distance functions and Different K  for Breast cancer


"""

# test the euclidean
euclidean_test_accs = [0]
euclidean_train_accs = [0]

K = range(1, 10)

for k in K:
  model = KNN(k,dist_fn=euclidean)
  model = model.fit(breast_x_train, breast_y_train)
  probs_test, knns = model.predict(breast_x_test)
  probs_train, knns = model.predict(breast_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(breast_y_test, y_pred_test)
  train_accuracy = evaluate_acc(breast_y_train, y_pred_train)
  euclidean_test_accs.append(test_accuracy)
  euclidean_train_accs.append(train_accuracy)
print(knns,knns.shape)
plt.xticks(range(0,10,1))
plt.title("Accuracy using euclidean function")
plt.plot(euclidean_test_accs, '-', label="Test acc")
plt.plot(euclidean_train_accs,'-', label="Train acc")
plt.legend()
plt.show()
print(f"largest accuracy when K = {euclidean_test_accs.index(max(euclidean_test_accs))}, the accuracy is {max(euclidean_test_accs)*100:.1f}\n")
############################################################################

# test the manhattan
manhattan_test_accs = [0]
manhattan_train_accs = [0]
K = range(1, 10)

for k in K:
  model = KNN(k,dist_fn=manhattan)
  model = model.fit(breast_x_train, breast_y_train)
  probs_test, knns = model.predict(breast_x_test)
  probs_train, knns = model.predict(breast_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(breast_y_test, y_pred_test)
  train_accuracy = evaluate_acc(breast_y_train, y_pred_train)
  manhattan_test_accs.append(test_accuracy)
  manhattan_train_accs.append(train_accuracy)

plt.xticks(range(0,10,1))
plt.title("Accuracy using manhattan function")
plt.plot(manhattan_test_accs, '-', label="Test acc")
plt.plot(manhattan_train_accs,'-', label="Train acc")
plt.legend()
plt.show()
print(f"largest accuracy when K = {manhattan_test_accs.index(max(manhattan_test_accs))}, the accuracy is {max(manhattan_test_accs)*100:.1f}\n")

############################################################################
# test the minkowski_p3
minkowski_p3_test_accs = [0]
minkowski_p3_train_accs = [0]
K = range(1, 10)

for k in K:
  model = KNN(k,dist_fn=minkowski_p3)
  model = model.fit(breast_x_train, breast_y_train)
  probs_test, knns = model.predict(breast_x_test)
  probs_train, knns = model.predict(breast_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(breast_y_test, y_pred_test)
  train_accuracy = evaluate_acc(breast_y_train, y_pred_train)
  minkowski_p3_test_accs.append(test_accuracy)
  minkowski_p3_train_accs.append(train_accuracy)

plt.xticks(range(0,10,1))
plt.title("Accuracy using minkowski_p3 function")
plt.plot(minkowski_p3_test_accs, '-', label="Test acc")
plt.plot(minkowski_p3_train_accs,'-', label="Train acc")
plt.legend()
plt.show()
print(f"largest accuracy when K = {minkowski_p3_test_accs.index(max(minkowski_p3_test_accs))}, the accuracy is {max(minkowski_p3_test_accs)*100:.1f}\n")

"""# Different distance functions and Different K  for hepatitis"""

from sklearn.metrics import confusion_matrix
#distance functions and accuracy with hepatitis_dataset
print("\nWith the hepatitis_dataset")

# test the euclidean
euclidean_test_accs = [0]
euclidean_train_accs = [0]
euclidean_test_accs_fs = [0]
euclidean_train_accs_fs = [0]
K = range(1, 25)

for k in K:
  model = KNN(k,dist_fn=euclidean)
  model = model.fit(hepatitis_x_train, hepatitis_y_train)
  probs_test, knns = model.predict(hepatitis_x_test)
  probs_train, knns_ = model.predict(hepatitis_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test)
  # test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train)
  euclidean_test_accs.append(test_accuracy)
  euclidean_train_accs.append(train_accuracy)

  model_fs = KNN(k,dist_fn=euclidean)
  model_fs = model_fs.fit(hepatitis_x_train_fs, hepatitis_y_train)
  probs_test_fs, knn_fs = model_fs.predict(hepatitis_x_test_fs)
  probs_train_fs, knn_fs_ = model_fs.predict(hepatitis_x_train_fs)
  y_pred_test_fs = np.argmax(probs_test_fs,1)
  y_pred_train_fs = np.argmax(probs_train_fs, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test_fs)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train_fs)
  euclidean_test_accs_fs.append(test_accuracy)
  euclidean_train_accs_fs.append(train_accuracy)

plt.xticks(range(0,25,1))
plt.title("Accuracy using euclidean function")
plt.plot(euclidean_test_accs, '-', label="Test acc")
plt.plot(euclidean_train_accs,'-', label="Train acc")
plt.plot(euclidean_test_accs_fs, '-', label="Test acc after fs")
plt.plot(euclidean_train_accs_fs,'-', label="Train acc after fs")
plt.legend()
plt.show()
print(f"largest accuracy when K = {euclidean_test_accs.index(max(euclidean_test_accs))}, the accuracy is {max(euclidean_test_accs)*100:.1f}\n")
print(f"largest test accuracy (feature selection) when K = {euclidean_test_accs_fs.index(max(euclidean_test_accs_fs))}, the accuracy is {max(euclidean_test_accs_fs)*100:.1f}\n")
print("confusion matrix:")
print(confusion_matrix(hepatitis_y_test, y_pred_test_fs))
############################################################################

# test the manhattan
manhattan_test_accs = [0]
manhattan_train_accs = [0]
manhattan_train_accs_fs = [0]
manhattan_test_accs_fs = [0]
K = range(1, 25)

for k in K:
  model = KNN(k,dist_fn=manhattan)
  model = model.fit(hepatitis_x_train, hepatitis_y_train)
  probs_test, knns = model.predict(hepatitis_x_test)
  probs_train, knns = model.predict(hepatitis_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train)
  manhattan_test_accs.append(test_accuracy)
  manhattan_train_accs.append(train_accuracy)

  model_fs = KNN(k,dist_fn=manhattan)
  model_fs = model_fs.fit(hepatitis_x_train_fs, hepatitis_y_train)
  probs_test_fs, knn_fs = model_fs.predict(hepatitis_x_test_fs)
  probs_train_fs, knn_fs_ = model_fs.predict(hepatitis_x_train_fs)
  y_pred_test_fs = np.argmax(probs_test_fs,1)
  y_pred_train_fs = np.argmax(probs_train_fs, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test_fs)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train_fs)
  manhattan_test_accs_fs.append(test_accuracy)
  manhattan_train_accs_fs.append(train_accuracy)


plt.xticks(range(0,25,1))
plt.title("Accuracy using manhattan function")
plt.plot(manhattan_test_accs, '-', label="Test acc")
plt.plot(manhattan_train_accs,'-', label="Train acc")
plt.plot(manhattan_test_accs_fs, '-', label="Test acc after fs")
plt.plot(manhattan_train_accs_fs,'-', label="Train acc after fs")
plt.legend()
plt.show()
print(f"largest accuracy when K = {manhattan_test_accs.index(max(manhattan_test_accs))}, the accuracy is {max(manhattan_test_accs)*100:.1f}\n")
print(f"largest test accuracy (feature selection) when K = {manhattan_test_accs_fs.index(max(manhattan_test_accs_fs))}, the accuracy is {max(manhattan_test_accs_fs)*100:.1f}\n")
print("confusion matrix:")
print(confusion_matrix(hepatitis_y_test, y_pred_test_fs))
############################################################################

# test the minkowski_p3
minkowski_p3_test_accs = [0]
minkowski_p3_train_accs = [0]
minkowski_p3_test_accs_fs = [0]
minkowski_p3_train_accs_fs = [0]
K = range(1, 10)

for k in K:
  model = KNN(k,dist_fn=minkowski_p3)
  model = model.fit(hepatitis_x_train, hepatitis_y_train)
  probs_test, knns = model.predict(hepatitis_x_test)
  probs_train, knns = model.predict(hepatitis_x_train)
  y_pred_test = np.argmax(probs_test,1)
  y_pred_train = np.argmax(probs_train, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train)
  minkowski_p3_test_accs.append(test_accuracy)
  minkowski_p3_train_accs.append(train_accuracy)

  model_fs = KNN(k,dist_fn=minkowski_p3)
  model_fs = model_fs.fit(hepatitis_x_train_fs, hepatitis_y_train)
  probs_test_fs, knn_fs = model_fs.predict(hepatitis_x_test_fs)
  probs_train_fs, knn_fs_ = model_fs.predict(hepatitis_x_train_fs)
  y_pred_test_fs = np.argmax(probs_test_fs,1)
  y_pred_train_fs = np.argmax(probs_train_fs, 1)
  test_accuracy = evaluate_acc(hepatitis_y_test, y_pred_test_fs)
  train_accuracy = evaluate_acc(hepatitis_y_train, y_pred_train_fs)
  minkowski_p3_test_accs_fs.append(test_accuracy)
  minkowski_p3_train_accs_fs.append(train_accuracy)

plt.xticks(range(0,10,1))
plt.title("Accuracy using minkowski function")
plt.plot(minkowski_p3_test_accs_fs, '-', label="Test acc after fs")
plt.plot(minkowski_p3_train_accs_fs,'-', label="Train acc after fs")
plt.plot(minkowski_p3_test_accs, '-', label="Test acc")
plt.plot(minkowski_p3_train_accs,'-', label="Train acc")
plt.legend()
plt.show()
print(f"largest accuracy when K = {minkowski_p3_test_accs.index(max(minkowski_p3_test_accs))}, the accuracy is {max(minkowski_p3_test_accs)*100:.1f}\n")
print(f"largest accuracy (feature selection) when K = {minkowski_p3_test_accs_fs.index(max(minkowski_p3_test_accs_fs))}, the accuracy is {max(minkowski_p3_test_accs_fs)*100:.1f}\n")
#confusion matrix
print("confusion matrix:")
print(confusion_matrix(hepatitis_y_test, y_pred_test_fs))

"""## Decision Boundaries for Breast
To draw the decision boundary we classify all the points on a 2D grid. The `meshgrid` function creates all the points on the grid by taking discretizations of horizontal and vertical axes.
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

std_breast_x = StandardScaler().fit_transform(breast_x)

b_pca=PCA(n_components=2,copy=True)
b_principal_components = b_pca.fit_transform(std_breast_x)

std_breast_x_train = b_principal_components[breast_inds[:breast_partition]]

#we can make the grid finer by increasing the number of samples from 200 to higher value
x0v = np.linspace(np.min(b_principal_components[:,0]), np.max(b_principal_components[:,0]), 250)
x1v = np.linspace(np.min(b_principal_components[:,1]), np.max(b_principal_components[:,1]), 250)

#to features values as a mesh  
x0, x1 = np.meshgrid(x0v, x1v)
x_all = np.vstack((x0.ravel(),x1.ravel())).T

C = int(np.max(breast_y))+1

print("using euclidean to draw boundaries")
for k in range(1,7):
  model = KNN(k,  euclidean)   #using euclidean to draw boundaries

  y_train_prob = np.zeros((breast_y_train.shape[0], C))
  y_train_prob[np.arange(breast_y_train.shape[0]), breast_y_train] = 1
  

  #to get class probability of all the points in the 2D grid
  y_prob_all, knns = model.fit(std_breast_x_train, breast_y_train).predict(x_all)

  y_pred_all = np.zeros_like(y_prob_all)
  y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1

  plt.scatter(std_breast_x_train[:,0], std_breast_x_train[:,1], c=y_train_prob, marker='o', alpha=1)
  plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)
  plt.title(f"with K={k}")
  plt.ylabel('Principle Component 0')
  plt.xlabel('Principle Component 1')
  plt.show()

################################
print("********************************************************************************")
print("using manhattan to draw boundaries")
for k in range(1,7):
  model = KNN(k, manhattan)   #using manhattan to draw boundaries

  y_train_prob = np.zeros((breast_y_train.shape[0], C))
  y_train_prob[np.arange(breast_y_train.shape[0]), breast_y_train] = 1
  
  #to get class probability of all the points in the 2D grid
  y_prob_all, knns = model.fit(std_breast_x_train, breast_y_train).predict(x_all)

  y_pred_all = np.zeros_like(y_prob_all)
  y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1

  plt.scatter(std_breast_x_train[:,0], std_breast_x_train[:,1], c=y_train_prob, marker='o', alpha=1)
  plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)
  plt.title(f"with K={k}")
  plt.ylabel('Principle Component 0')
  plt.xlabel('Principle Component 1')
  plt.show()

"""## Decision Boundaries for Hepatitis

To draw the decision boundary we classify all the points on a 2D grid. The `meshgrid` function creates all the points on the grid by taking discretizations of horizontal and vertical axes.
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

std_hepatitis_x = StandardScaler().fit_transform(hepatitis_x)

h_pca=PCA(n_components=2,copy=True)
h_principal_components = h_pca.fit_transform(std_hepatitis_x)

std_hepatitis_x_train = h_principal_components[hepatitis_inds[:hepatitis_partition]]

#we can make the grid finer by increasing the number of samples from 200 to higher value
x0v = np.linspace(np.min(h_principal_components[:,0]), np.max(h_principal_components[:,0]), 250)
x1v = np.linspace(np.min(h_principal_components[:,1]), np.max(h_principal_components[:,1]), 250)

#to features values as a mesh  
x0, x1 = np.meshgrid(x0v, x1v)
x_all = np.vstack((x0.ravel(),x1.ravel())).T

C = int(np.max(hepatitis_y))+1

print("using euclidean to draw boundaries")
for k in range(1,7):
  model = KNN(k, euclidean)   #using euclidean to draw boundaries

  y_train_prob = np.zeros((hepatitis_y_train.shape[0], C))
  y_train_prob[np.arange(hepatitis_y_train.shape[0]), hepatitis_y_train] = 1
  

  #to get class probability of all the points in the 2D grid
  y_prob_all, knns = model.fit(std_hepatitis_x_train, hepatitis_y_train).predict(x_all)

  y_pred_all = np.zeros_like(y_prob_all)
  y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1

  plt.scatter(std_hepatitis_x_train[:,0], std_hepatitis_x_train[:,1], c=y_train_prob, marker='o', alpha=1)
  plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)
  plt.title(f"with K={k}")
  plt.ylabel('Principle Component 0')
  plt.xlabel('Principle Component 1')
  plt.show()

################################
print("********************************************************************************")
print("using manhattan to draw boundaries")
for k in range(1,7):
  model = KNN(k, manhattan)   #using manhattan to draw boundaries

  y_train_prob = np.zeros((hepatitis_y_train.shape[0], C))
  y_train_prob[np.arange(hepatitis_y_train.shape[0]), hepatitis_y_train] = 1
  

  #to get class probability of all the points in the 2D grid
  y_prob_all, knns = model.fit(std_hepatitis_x_train, hepatitis_y_train).predict(x_all)

  y_pred_all = np.zeros_like(y_prob_all)
  y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1

  plt.scatter(std_hepatitis_x_train[:,0], std_hepatitis_x_train[:,1], c=y_train_prob, marker='o', alpha=1)
  plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)
  plt.title(f"with K={k}")
  plt.ylabel('Principle Component 0')
  plt.xlabel('Principle Component 1')
  plt.show()